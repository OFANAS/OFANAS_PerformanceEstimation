{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Get Your Specialized Neural Networks on ImageNet in Minutes With OFA Networks\n",
    "\n",
    "In this notebook, we will demonstrate \n",
    "- how to use pretrained specialized OFA sub-networks for efficient inference on diverse hardware platforms\n",
    "- how to get new specialized neural networks on ImageNet with the OFA network within minutes.\n",
    "\n",
    "**[Once-for-All (OFA)](https://github.com/mit-han-lab/once-for-all)** is an efficient AutoML technique\n",
    "that decouples training from search.\n",
    "Different sub-nets can directly grab weights from the OFA network without training.\n",
    "Therefore, getting a new specialized neural network with the OFA network is highly efficient, incurring little computation cost.\n",
    "\n",
    "![](https://hanlab.mit.edu/files/OnceForAll/figures/ofa_search_cost.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation\n",
    "Let's first install all the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Installing PyTorch...')\n",
    "! pip install torch 1>/dev/null\n",
    "print('Installing torchvision...')\n",
    "! pip install torchvision 1>/dev/null\n",
    "print('Installing numpy...')\n",
    "! pip install numpy 1>/dev/null\n",
    "# thop is a package for FLOPs computing.\n",
    "print('Installing thop (FLOPs counter) ...')\n",
    "! pip install thop 1>/dev/null\n",
    "# ofa is a package containing training code, pretrained specialized models and inference code for the once-for-all networks.\n",
    "print('Installing OFA...')\n",
    "! pip install ofa 1>/dev/null\n",
    "# tqdm is a package for displaying a progress bar.\n",
    "print('Installing tqdm (progress bar) ...')\n",
    "! pip install tqdm 1>/dev/null\n",
    "print('Installing matplotlib...')\n",
    "! pip install matplotlib 1>/dev/null\n",
    "print('All required packages have been successfully installed!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can import the packages used in this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully imported all packages and configured random seed to 1!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import copy\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from ofa.model_zoo import ofa_net\n",
    "from ofa.utils import download_url\n",
    "\n",
    "from accuracy_predictor import AccuracyPredictor\n",
    "from flops_table import FLOPsTable\n",
    "from latency_table import LatencyTable\n",
    "from evolution_finder import EvolutionFinder\n",
    "from imagenet_eval_helper import evaluate_ofa_subnet, evaluate_ofa_specialized\n",
    "\n",
    "# set random seed\n",
    "random_seed = 1\n",
    "random.seed(random_seed)\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "print('Successfully imported all packages and configured random seed to %d!'%random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to determine which device to use for neural network inference in the rest of this tutorial. If your machine is equipped with GPU(s), we will use the GPU by default. Otherwise, we will use the CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU.\n"
     ]
    }
   ],
   "source": [
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "cuda_available = torch.cuda.is_available()\n",
    "if cuda_available:\n",
    "    torch.backends.cudnn.enabled = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    torch.cuda.manual_seed(random_seed)\n",
    "    print('Using GPU.')\n",
    "else:\n",
    "    print('Using CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! Now you have successfully configured the environment! It's time to import the **OFA network** for the following experiments.\n",
    "The OFA network used in this tutorial is built upon MobileNetV3 with width multiplier 1.2, supporting elastic depth (2, 3, 4) per stage, elastic expand ratio (3, 4, 6), and elastic kernel size (3, 5 7) per block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The OFA Network is ready.\n"
     ]
    }
   ],
   "source": [
    "ofa_network = ofa_net('ofa_mbv3_d234_e346_k357_w1.0', pretrained=True)\n",
    "# ofa_network_2 = ofa_net('ofa', pretrained=True)\n",
    "print('The OFA Network is ready.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, let's build the ImageNet dataset and the corresponding dataloader. Notice that **if you're using the CPU,\n",
    "we will skip ImageNet evaluation by default** since it will be very slow.\n",
    "If you are using the GPU, in case you don't have the full dataset,\n",
    "we will download a subset of ImageNet which contains 2,000 images (~250M) for testing.\n",
    "If you do have the full ImageNet dataset on your machine, just specify it in `imagenet_data_path` and the downloading script will be skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.\n"
     ]
    }
   ],
   "source": [
    "if cuda_available:\n",
    "    # path to the ImageNet dataset\n",
    "    #print(\"Please input the path to the ImageNet dataset.\\n\")\n",
    "    imagenet_data_path = '/netnvme/msahni7/ImageNet'\n",
    "\n",
    "    # if 'imagenet_data_path' is empty, download a subset of ImageNet containing 2000 images (~250M) for test\n",
    "    if not os.path.isdir(imagenet_data_path):\n",
    "        os.makedirs(imagenet_data_path, exist_ok=True)\n",
    "        download_url('https://hanlab.mit.edu/files/OnceForAll/ofa_cvpr_tutorial/imagenet_1k.zip', model_dir='data')\n",
    "        ! cd data && unzip imagenet_1k 1>/dev/null && cd ..\n",
    "        ! cp -r data/imagenet_1k/* $imagenet_data_path\n",
    "        ! rm -rf data\n",
    "        print('%s is empty. Download a subset of ImageNet for test.' % imagenet_data_path)\n",
    "\n",
    "    print('The ImageNet dataset files are ready.')\n",
    "else:\n",
    "    print('Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have configured the dataset. Let's build the dataloader for evaluation.\n",
    "Again, this will be skipped if you are in a CPU environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.\n"
     ]
    }
   ],
   "source": [
    "if cuda_available:\n",
    "    # The following function build the data transforms for test\n",
    "    def build_val_transform(size):\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize(int(math.ceil(size / 0.875))),\n",
    "            transforms.CenterCrop(size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225]\n",
    "            ),\n",
    "        ])\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(\n",
    "            root=os.path.join(imagenet_data_path, 'val'),\n",
    "            transform=build_val_transform(224)\n",
    "        ),\n",
    "        batch_size=250,  # test batch size\n",
    "        shuffle=True,\n",
    "        num_workers=16,  # number of workers for the data loader\n",
    "        pin_memory=True,\n",
    "        drop_last=False,\n",
    "    )\n",
    "    print('The ImageNet dataloader is ready.')\n",
    "else:\n",
    "    data_loader = None\n",
    "    print('Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Using Pretrained Specialized OFA Sub-Networks\n",
    "![](https://hanlab.mit.edu/files/OnceForAll/figures/select_subnets.png)\n",
    "The specialized OFA sub-networks are \"small\" networks sampled from the \"big\" OFA network as is indicated in the figure above.\n",
    "The OFA network supports over $10^{19}$ sub-networks simultaneously, so that the deployment cost for multiple scenarios can be saved by 16$\\times$ to 1300$\\times$ under 40 deployment scenarios.\n",
    "Now, let's play with some of the sub-networks through the following interactive command line prompt (**Notice that for CPU users, this will be skipped**).\n",
    "We recommend you to try a smaller sub-network (e.g., the sub-network for pixel1 with 20ms inference latency constraint) so that it takes less time to evaluate the model on ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.\n"
     ]
    }
   ],
   "source": [
    "if cuda_available:\n",
    "    net_id = evaluate_ofa_specialized(imagenet_data_path, data_loader)\n",
    "    print('Finished evaluating the pretrained sub-network: %s!' % net_id)\n",
    "else:\n",
    "    print('Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Efficient Deployment with OFA Networks\n",
    "\n",
    "You have now successfully prepared the whole environment for the experiment!\n",
    "In the next step, we will introduce **how to get efficient, specialized neural networks within minutes**\n",
    "powered by the OFA network.\n",
    "\n",
    "### 3.1 Latency-Constrained Efficient Deployment on Samsung Note10\n",
    "\n",
    "The key components of very fast neural network deployment are **accuracy predictors** and **efficiency predictors**.\n",
    "For the accuracy predictor, it predicts the Top-1 accuracy of a given sub-network on a **holdout validation set**\n",
    "(different from the official 50K validation set) so that we do **NOT** need to run very costly inference on ImageNet\n",
    "while searching for specialized models. Such an accuracy predictor is trained using an accuracy dataset built with the OFA network.\n",
    "\n",
    "![](https://hanlab.mit.edu/files/OnceForAll/figures/predictor_based_search.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy predictor is ready!\n",
      "Sequential(\n",
      "  (0): Linear(in_features=128, out_features=400, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=400, out_features=400, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=400, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# accuracy predictor\n",
    "accuracy_predictor = AccuracyPredictor(\n",
    "    pretrained=True,\n",
    "    device='cuda:0' if cuda_available else 'cpu'\n",
    ")\n",
    "\n",
    "print('The accuracy predictor is ready!')\n",
    "print(accuracy_predictor.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the powerful **accuracy predictor**. We then introduce two types of **efficiency predictors**: the latency predictor and the FLOPs predictor. \n",
    "\n",
    "The intuition of having efficiency predictors, especially the latency predictor, is that measuring the latency of a sub-network on-the-fly is also costly, especially for mobile devices.\n",
    "The latency predictor is designed to eliminate this cost.\n",
    "Let's load a latency predictor we built beforehand for the Samsung Note10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Latency lookup table on gpu is ready!\n"
     ]
    }
   ],
   "source": [
    "target_hardware = 'gpu'\n",
    "latency_table = LatencyTable(device=target_hardware, use_latency_table=False)\n",
    "print('The Latency lookup table on %s is ready!' % target_hardware)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have defined both the accuracy predictor and the latency predictor. Now, let's experience **very fast model specialization** on Samsung Note10 with these two powerful predictors! \n",
    "\n",
    "**Notice**: The predicted accuracy is on a holdout validation set of 10K images, not the official 50K validation set.\n",
    "But they are highly positive-correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top1s_1 = [73.90200145721435, 75.10800159454345, 77.07400157928467, 77.49000156402587, 78.13600151062012, 78.43400157928467, 78.67600173950196, 78.93600147247315, 78.83800159454346, 79.16600154876708]\n",
    "#latency_list_1 = [14.974026126766184, 19.842021273273765, 24.97401650579812, 29.916699479534902, 34.42456262829833, 39.79864816264132, 44.29353466608003, 49.66494550464141, 54.77624689768104, 59.74252719184961]\n",
    "\n",
    "top1s_1 = [71.99200141906738, 73.83400161743164, 75.08400173187256, 75.56600158691407, 75.79200138092041, 76.46200187683105, 77.00800132751465, 77.09600151062011, 77.09200138092041, 77.27400165557862]\n",
    "latency_list_1 = [14.915969134430984, 19.999238905682866, 24.74897687431021, 29.84660924139936, 34.780664078300624, 39.63810944460619, 44.780066593130584, 49.68762514010865, 54.796040548223594, 59.90540405076071]\n",
    "\n",
    "top1s_2 = [71.85800170898438, 73.72600173950195, 74.81200164794922, 75.20200160980225, 75.73600162506104, 76.26200149536133, 76.32400157928467, 76.70000144958496, 76.93400157928467, 76.93000144958496]\n",
    "latency_list_2 = [14.69766178609321, 19.88688251867145, 24.83390000854306, 29.512724161788654, 34.79207886302561, 38.776624637240026, 43.77618481697257, 48.95272096675038, 52.47479555325202, 52.47479555325202]\n",
    "\n",
    "# top1s_2 =  [72.16600151062012, 73.92600162506103, 74.68800144195556, 75.2400016784668, 76.05800186157227, 76.72000164031982, 76.90000129699708, 77.15400131225586, 77.36200130462646, 77.66800140380859]\n",
    "# latency_list_2 = [14.883577352241502, 19.985920805125158, 24.889105597347815, 28.7295330439169, 34.834378690071375, 39.653604946621535, 43.948637332259246, 49.431102985143454, 54.927527569742765, 58.28524706388474]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%    \n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Hyper-parameters for the evolutionary search process\n",
    "    You can modify these hyper-parameters to see how they influence the final ImageNet accuracy of the search sub-net.\n",
    "\"\"\"\n",
    "latency_constraint = 25  # ms, suggested range [15, 33] ms\n",
    "P = 100  # The size of population in each generation\n",
    "N = 500  # How many generations of population to be searched\n",
    "r = 0.25  # The ratio of networks that are used as parents for next generation\n",
    "params = {\n",
    "    'constraint_type': target_hardware, # Let's do FLOPs-constrained search\n",
    "    'efficiency_constraint': latency_constraint,\n",
    "    'mutate_prob': 0.1, # The probability of mutation in evolutionary search\n",
    "    'mutation_ratio': 0.5, # The ratio of networks that are generated through mutation in generation n >= 2.\n",
    "    'efficiency_predictor': latency_table, # To use a predefined efficiency predictor.\n",
    "    'accuracy_predictor': accuracy_predictor, # To use a predefined accuracy_predictor predictor.\n",
    "    'population_size': P,\n",
    "    'max_time_budget': N,\n",
    "    'parent_ratio': r,\n",
    "    'arch' : 'compofa', ## change\n",
    "}\n",
    "\n",
    "# build the evolution finder\n",
    "finder = EvolutionFinder(**params)\n",
    "\n",
    "\"\"\"\n",
    "# start searching\n",
    "result_lis = []\n",
    "st = time.time()\n",
    "best_valids, best_info = finder.run_evolution_search()\n",
    "result_lis.append(best_info)\n",
    "ed = time.time()\n",
    "print('Found best architecture on %s with latency <= %.2f ms in %.2f seconds! '\n",
    "      'It achieves %.2f%s predicted accuracy with %.2f ms latency on %s.' %\n",
    "      (target_hardware, latency_constraint, ed-st, best_info[0] * 100, '%', best_info[-1], target_hardware))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "result_lis = []\n",
    "for latency in [15, 20, 25, 30]:#, 35, 40, 45, 50, 55, 60]:\n",
    "    st = time.time()\n",
    "    finder.set_efficiency_constraint(latency)\n",
    "    best_valids, best_info = finder.run_evolution_search()\n",
    "    ed = time.time()\n",
    "    result_lis.append(best_info)\n",
    "print(\"Done!\")\n",
    "\n",
    "\"\"\"\n",
    "# visualize the architecture of the searched sub-net\n",
    "_, net_config, latency = best_info\n",
    "ofa_network.set_active_subnet(ks=net_config['ks'], d=net_config['d'], e=net_config['e'])\n",
    "print('Architecture of the searched sub-net:')\n",
    "print(ofa_network.module_str)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ed-st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lis = [(0.7825909852981567, {'wid': None, 'ks': [5, 3, 7, 7, 5, 3, 3, 3, 5, 3, 5, 7, 3, 5, 5, 3, 3, 3, 3, 5], 'e': [3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], 'd': [2, 2, 3, 3, 3], 'r': [160]}, 14.883577352241502), (0.8028315901756287, {'wid': None, 'ks': [3, 3, 7, 7, 7, 3, 5, 3, 5, 3, 7, 3, 7, 5, 3, 5, 3, 3, 3, 5], 'e': [3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], 'd': [2, 2, 3, 3, 3], 'r': [192]}, 19.985920805125158), (0.8108872175216675, {'wid': None, 'ks': [3, 3, 5, 3, 7, 3, 3, 3, 5, 5, 3, 3, 5, 7, 7, 3, 3, 3, 7, 7], 'e': [3, 3, 3, 3, 4, 4, 4, 4, 6, 6, 6, 6, 4, 4, 4, 4, 4, 4, 4, 4], 'd': [2, 3, 4, 3, 3], 'r': [192]}, 24.889105597347815), (0.8177733421325684, {'wid': None, 'ks': [5, 5, 3, 5, 7, 5, 3, 3, 7, 7, 3, 7, 7, 5, 3, 3, 7, 7, 3, 5], 'e': [3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4], 'd': [2, 3, 3, 3, 3], 'r': [208]}, 28.7295330439169), (0.8261815905570984, {'wid': None, 'ks': [3, 5, 7, 3, 3, 3, 3, 3, 7, 7, 7, 3, 5, 3, 7, 5, 3, 7, 5, 7], 'e': [3, 3, 3, 3, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], 'd': [2, 3, 4, 4, 4], 'r': [192]}, 34.834378690071375), (0.8325710892677307, {'wid': None, 'ks': [5, 5, 3, 3, 5, 3, 5, 3, 7, 7, 3, 3, 7, 5, 7, 3, 3, 7, 5, 5], 'e': [3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6], 'd': [2, 3, 3, 4, 4], 'r': [208]}, 39.653604946621535), (0.834993302822113, {'wid': None, 'ks': [5, 5, 5, 3, 5, 3, 3, 5, 7, 7, 3, 3, 7, 5, 3, 7, 5, 7, 5, 5], 'e': [3, 3, 3, 3, 6, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6], 'd': [2, 4, 3, 4, 4], 'r': [208]}, 43.948637332259246), (0.839465856552124, {'wid': None, 'ks': [3, 7, 7, 5, 5, 7, 5, 3, 5, 7, 7, 7, 5, 3, 5, 5, 3, 7, 5, 5], 'e': [3, 3, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], 'd': [2, 4, 4, 4, 4], 'r': [208]}, 49.431102985143454), (0.8430442810058594, {'wid': None, 'ks': [5, 5, 7, 3, 5, 7, 3, 3, 3, 7, 5, 7, 5, 3, 7, 5, 3, 7, 5, 3], 'e': [4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], 'd': [3, 4, 4, 4, 4], 'r': [208]}, 54.927527569742765), (0.844395101070404, {'wid': None, 'ks': [5, 5, 5, 5, 3, 5, 3, 3, 5, 5, 5, 7, 7, 7, 3, 5, 3, 3, 5, 5], 'e': [4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6], 'd': [3, 4, 4, 4, 4], 'r': [224]}, 58.28524706388474)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result_lis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You get your specialized neural network with **just a few seconds**!\n",
    "You can go back to the last cell and modify the hyper-parameters to see how they affect the search time and the accuracy.\n",
    "\n",
    "We also provided an interface below to draw a figure comparing your searched specialized network and other efficient neural networks such as MobileNetV3 and ProxylessNAS.\n",
    "\n",
    "**Notice**: For ease of comparison, we recommend you to choose a latency constraint between 15ms and 33ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the searched model on ImageNet\n",
    "if cuda_available:\n",
    "    top1s_2 = []             ## change\n",
    "    latency_list_2 = []      ## change\n",
    "    for result in result_lis:\n",
    "        _, net_config, latency = result\n",
    "        print('Evaluating the sub-network with latency = %.1f ms on %s' % (latency, target_hardware))\n",
    "        top1 = evaluate_ofa_subnet(\n",
    "            ofa_network_2,                    ## change\n",
    "            imagenet_data_path,\n",
    "            net_config,\n",
    "            data_loader,\n",
    "            batch_size=250,\n",
    "            device='cuda:0' if cuda_available else 'cpu')\n",
    "        top1s_2.append(top1)                ## change\n",
    "        latency_list_2.append(latency)      ## change\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.plot(latency_list_2, top1s_2, 'x-', marker='o', color='red',  linewidth=2, markersize=8, label='Comp_OFA')\n",
    "    plt.plot(latency_list_1, top1s_1, 'x-', marker='*', color='darkred',  linewidth=2, markersize=8, label='OFA')\n",
    "    plt.plot([26, 45], [74.6, 76.7], '--', marker='+', linewidth=2, markersize=8, label='ProxylessNAS')\n",
    "    plt.plot([15.3, 22, 31], [73.3, 75.2, 76.6], '--', marker='>', linewidth=2, markersize=8, label='MobileNetV3')\n",
    "    plt.xlabel('%s Latency (ms)' % target_hardware, size=12)\n",
    "    plt.ylabel('ImageNet Top-1 Accuracy (%)', size=12)\n",
    "    plt.legend(['CompOFA', 'OFA', 'ProxylessNAS', 'MobileNetV3'], loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print('Successfully draw the tradeoff curve!')\n",
    "else:\n",
    "    print('Since GPU is not found in the environment, we skip all scripts related to ImageNet evaluation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top1s_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(latency_list_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** You can further significantly improve the accuracy of the searched sub-net by fine-tuning it on the ImageNet training set.\n",
    "Our results after fine-tuning for 25 epochs are as follows:\n",
    "![](https://hanlab.mit.edu/files/OnceForAll/figures/diverse_hardware.png)\n",
    "\n",
    "\n",
    "### 3.2 FLOPs-Constrained Efficient Deployment\n",
    "\n",
    "Now, let's proceed to the final experiment of this tutorial: efficient deployment under FLOPs constraint. We use the same accuracy predictor since accuracy predictors are agnostic to the types of efficiency constraint (mobile latency / FLOPs). For the efficiency predictor, we change the latency lookup table to a flops lookup table. You can run the code below to setup it in a few seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "flops_lookup_table = FLOPsTable(\n",
    "    device='cuda:0' if cuda_available else 'cpu',\n",
    "    batch_size=1,\n",
    ")\n",
    "print('The FLOPs lookup table is ready!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can start a FLOPs-constrained neural architecture search. Here, we directly generate **an entire tradeoff curve** for you. Please notice that the time it takes to get each data point will get longer and longer (but always less than 30 seconds) because smaller FLOPs-constraint is more difficult to meet.\n",
    "\n",
    "If you are using CPUs, you will be able to see a \"predicted holdout validation set accuracy - FLOPs\" tradeoff curve, which can be obtained in just a  minute.\n",
    "\n",
    "If you are using GPUs, besides the curve mentioned above, we will also evaluate all the models you designed on the ImageNet validation set (**Again, it will be better if you have the full ImageNet validation set**, but it's also OK if you downloaded the subset above) and generate an \"ImageNet 50K validation set accuracy - FLOPs\" tradeoff curve. We will also plot competing methods such as ProxylessNAS, MobileNetV3, and EfficientNet in this curve for your reference. The estimated time to get the two curves is less than 10 minutes.\n",
    "\n",
    "Please notice that it usually takes ** hundreds/thousands of hours** to generate an accuracy-FLOPs tradeoff curve for ProxylessNAS / MobileNetV3 / EfficientNet, but generating the tradeoff curve for our OFA takes just a few minutes, as you will experience soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\" Hyper-parameters for the evolutionary search process\n",
    "    You can modify these hyper-parameters to see how they influence the final ImageNet accuracy of the search sub-net.\n",
    "\"\"\"\n",
    "P = 100  # The size of population in each generation\n",
    "N = 500  # How many generations of population to be searched\n",
    "r = 0.25  # The ratio of networks that are used as parents for next generation\n",
    "params = {\n",
    "    'constraint_type': 'flops', # Let's do FLOPs-constrained search\n",
    "    'efficiency_constraint': 600,  # FLops constraint (M), suggested range [150, 600]\n",
    "    'mutate_prob': 0.1, # The probability of mutation in evolutionary search\n",
    "    'mutation_ratio': 0.5, # The ratio of networks that are generated through mutation in generation n >= 2.\n",
    "    'efficiency_predictor': flops_lookup_table, # To use a predefined efficiency predictor.\n",
    "    'accuracy_predictor': accuracy_predictor, # To use a predefined accuracy_predictor predictor.\n",
    "    'population_size': P,\n",
    "    'max_time_budget': N,\n",
    "    'parent_ratio': r,\n",
    "    'arch' : 'compofa' # change this if required\n",
    "}\n",
    "\n",
    "# build the evolution finder\n",
    "finder = EvolutionFinder(**params)\n",
    "\n",
    "# start searching\n",
    "result_lis = []\n",
    "for flops in [150, 250, 350, 400, 500, 600]:\n",
    "    st = time.time()\n",
    "    finder.set_efficiency_constraint(flops)\n",
    "    best_valids, best_info = finder.run_evolution_search()\n",
    "    ed = time.time()\n",
    "    # print('Found best architecture at flops <= %.2f M in %.2f seconds! It achieves %.2f%s predicted accuracy with %.2f MFLOPs.' % (flops, ed-st, best_info[0] * 100, '%', best_info[-1]))\n",
    "    result_lis.append(best_info)\n",
    "\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot([x[-1] for x in result_lis], [x[0] * 100 for x in result_lis], 'x-', marker='*', color='darkred',  linewidth=2, markersize=8, label='OFA')\n",
    "plt.xlabel('FLOPs (M)', size=12)\n",
    "plt.ylabel('Predicted Holdout Top-1 Accuracy (%)', size=12)\n",
    "plt.legend(['OFA'], loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_lis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the searched models on ImageNet if GPU is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if cuda_available:\n",
    "    # test the searched model on the test dataset (ImageNet val)\n",
    "    top1s = []\n",
    "    flops_lis = []\n",
    "    for result in result_lis:\n",
    "        _, net_config, flops = result\n",
    "        print('Evaluating the sub-network with FLOPs = %.1fM' % flops)\n",
    "        top1 = evaluate_ofa_subnet(\n",
    "            ofa_network,\n",
    "            imagenet_data_path,\n",
    "            net_config,\n",
    "            data_loader,\n",
    "            batch_size=250,\n",
    "            device='cuda:0' if cuda_available else 'cpu')\n",
    "        print('-' * 45)\n",
    "        top1s.append(top1)\n",
    "        flops_lis.append(flops)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot([x[-1] for x in result_lis], [x[0] * 100 for x in result_lis], 'x-', marker='*', color='darkred',  linewidth=2, markersize=8, label='OFA')\n",
    "    plt.xlabel('FLOPs (M)', size=12)\n",
    "    plt.ylabel('Predicted Holdout Top-1 Accuracy (%)', size=12)\n",
    "    plt.legend(['OFA'], loc='lower right')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(flops_lis, top1s, 'x-', marker='*', color='darkred',  linewidth=2, markersize=8, label='OFA')\n",
    "    plt.plot([320, 581], [74.6, 76.7], '--', marker='+', linewidth=2, markersize=8, label='ProxylessNAS')\n",
    "    plt.plot([219, 343], [75.2, 76.6], '--', marker='^', linewidth=2, markersize=8, label='MobileNetV3')\n",
    "    plt.plot([390, 700], [76.3, 78.8], '--', marker='>', linewidth=2, markersize=8, label='EfficientNet')\n",
    "    plt.xlabel('FLOPs (M)', size=12)\n",
    "    plt.ylabel('ImageNet Top-1 Accuracy (%)', size=12)\n",
    "    plt.legend(['OFA', 'ProxylessNAS', 'MobileNetV3', 'EfficientNet'], loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top1s)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:** Again, you can further improve the accuracy of the search sub-net by fine-tuning it on ImageNet.\n",
    "The final accuracy is much better than training the same architecture from scratch.\n",
    "Our results are as follows:\n",
    "![](https://hanlab.mit.edu/files/OnceForAll/figures/imagenet_80_acc.png)\n",
    "![](https://hanlab.mit.edu/files/OnceForAll/figures/cnn_imagenet_new.png)\n",
    "\n",
    "Congratulations! You've finished all the content of this tutorial!\n",
    "Hope you enjoy playing with the OFA Networks. If you are interested,  please refer to our paper and GitHub Repo for further details.\n",
    "\n",
    "## Reference\n",
    "[1] CVPR'20 tutorial: **AutoML for TinyML with Once-for-All Network**. [[talk]](https://www.youtube.com/watch?v=fptQ_eJ3Uc0&feature=youtu.be).\n",
    "\n",
    "[1] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang and Song Han.\n",
    "**Once for All: Train One Network and Specialize It for Efficient Deployment**. In *ICLR* 2020.\n",
    "[[paper]](https://arxiv.org/abs/1908.09791), [[code]](https://github.com/mit-han-lab/once-for-all), [[talk]](https://www.youtube.com/watch?v=a_OeT8MXzWI).\n",
    "\n",
    "[2] Han Cai, Ligeng Zhu and Song Han. **ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware**.\n",
    "In *ICLR* 2019. [[paper]](https://arxiv.org/abs/1812.00332), [[code]](https://github.com/MIT-HAN-LAB/ProxylessNAS).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
